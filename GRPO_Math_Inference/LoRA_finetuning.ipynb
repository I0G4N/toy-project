{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cc1dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/logan/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /home/logan/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 17:47:51,622 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /home/logan/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 17:47:54,186 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5553fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: b51c0e55-7ece-4e65-a001-a55c9440f49c)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 788c8c0f-dae1-40be-8ca2-ca5e81e6d888)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/main/README.md\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 5e243b60-039e-473f-8e5b-ac784c250eb8)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/main/README.md\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: cff0bb33-ebea-4ac8-ad3a-33451263327a)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/main/README.md\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 9072f7dc-0337-4bad-841b-07ec3a71a1ed)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/main/README.md\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: aa20a7af-fc41-4fbf-b373-ac4c4167ca13)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/main/README.md\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 61f658df-5f19-43ed-8148-70d6cae9d318)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/gsm8k.py\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 44c421d1-8e55-4b62-97a2-12cf571c14ce)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/gsm8k.py\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: ef525d60-9f99-40c7-baf0-0daed92e3f37)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/gsm8k.py\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 0a7f6192-83cb-4a31-a866-da3951873d2e)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/gsm8k.py\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: cd559c9b-6366-466a-9f34-47db796274d6)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/gsm8k.py\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: ea10244e-be6c-4c9a-a1c8-197267ad8f70)')' thrown while requesting HEAD https://huggingface.co/datasets/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/gsm8k.py\n",
      "Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'main' at /home/logan/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu Oct  9 16:36:03 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fda8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def create_prompt_formats(sample):\n",
    "    SYSTEM_PROMPT = \"\"\"You are Qwen, created by Alibaba Cloud. According to the question, please provide the user with detailed reasoning steps and answer in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n\"\"\"\n",
    "\n",
    "    reasoning_steps, answer = [p.strip() for p in sample[\"answer\"].split(\"####\")]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": sample[\"question\"] + \"\\n\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"<reasoning>\\n{reasoning_steps}\\n</reasoning>\\n<answer>\\n{answer}\\n</answer>\\n\"}\n",
    "        ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "        )\n",
    "\n",
    "    sample[\"formatted_prompt\"] = text\n",
    "\n",
    "    return sample\n",
    "\n",
    "def preprocess_batch(batch, tokenizer):\n",
    "    return tokenizer(batch[\"formatted_prompt\"], truncation=True)\n",
    "\n",
    "def preprocess_dataset(tokenizer, dataset):\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    _preprocessing_function = partial(preprocess_batch, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['question', 'answer']\n",
    "    )\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab885905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 15649.47 examples/s]\n",
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 10277.80 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 16708.46 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 9844.38 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['formatted_prompt', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 7473\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['formatted_prompt', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 1319\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = preprocess_dataset(tokenizer, data['train'])\n",
    "test_dataset = preprocess_dataset(tokenizer, data['test'])\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045c3c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlogan-zh-cai\u001b[0m (\u001b[33mlogan-cai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/logan/working/toy-project/GRPO_Math_Inference/wandb/run-20251009_175126-g5tmo7bh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/logan-cai/LongCoT_Math_Inference/runs/g5tmo7bh' target=\"_blank\">toasty-energy-12</a></strong> to <a href='https://wandb.ai/logan-cai/LongCoT_Math_Inference' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/logan-cai/LongCoT_Math_Inference' target=\"_blank\">https://wandb.ai/logan-cai/LongCoT_Math_Inference</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/logan-cai/LongCoT_Math_Inference/runs/g5tmo7bh' target=\"_blank\">https://wandb.ai/logan-cai/LongCoT_Math_Inference/runs/g5tmo7bh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/logan-cai/LongCoT_Math_Inference/runs/g5tmo7bh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f371e44bee0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"LongCoT_Math_Inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d90300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncating train dataset: 100%|██████████| 7473/7473 [00:00<00:00, 464707.17 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 236695.63 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1869' max='1869' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1869/1869 37:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.212300</td>\n",
       "      <td>1.428616</td>\n",
       "      <td>1.121758</td>\n",
       "      <td>51412.000000</td>\n",
       "      <td>0.702637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.422400</td>\n",
       "      <td>0.993301</td>\n",
       "      <td>0.577937</td>\n",
       "      <td>101426.000000</td>\n",
       "      <td>0.799841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>0.908663</td>\n",
       "      <td>0.531967</td>\n",
       "      <td>152072.000000</td>\n",
       "      <td>0.809796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.900655</td>\n",
       "      <td>0.527968</td>\n",
       "      <td>203340.000000</td>\n",
       "      <td>0.811724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.342100</td>\n",
       "      <td>0.914902</td>\n",
       "      <td>0.503196</td>\n",
       "      <td>254997.000000</td>\n",
       "      <td>0.811708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.394800</td>\n",
       "      <td>0.902828</td>\n",
       "      <td>0.517793</td>\n",
       "      <td>304049.000000</td>\n",
       "      <td>0.812669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.900480</td>\n",
       "      <td>0.510114</td>\n",
       "      <td>355207.000000</td>\n",
       "      <td>0.812872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>0.907529</td>\n",
       "      <td>0.507379</td>\n",
       "      <td>406049.000000</td>\n",
       "      <td>0.813435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.297900</td>\n",
       "      <td>0.894996</td>\n",
       "      <td>0.517039</td>\n",
       "      <td>456321.000000</td>\n",
       "      <td>0.813920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.253900</td>\n",
       "      <td>0.895167</td>\n",
       "      <td>0.505356</td>\n",
       "      <td>507899.000000</td>\n",
       "      <td>0.814151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.902708</td>\n",
       "      <td>0.504677</td>\n",
       "      <td>560046.000000</td>\n",
       "      <td>0.813787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.918062</td>\n",
       "      <td>0.493452</td>\n",
       "      <td>609690.000000</td>\n",
       "      <td>0.813046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.913375</td>\n",
       "      <td>0.498526</td>\n",
       "      <td>660486.000000</td>\n",
       "      <td>0.813772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.889588</td>\n",
       "      <td>0.505086</td>\n",
       "      <td>711843.000000</td>\n",
       "      <td>0.814443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>0.914962</td>\n",
       "      <td>0.491846</td>\n",
       "      <td>762309.000000</td>\n",
       "      <td>0.813875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.905099</td>\n",
       "      <td>0.500312</td>\n",
       "      <td>813360.000000</td>\n",
       "      <td>0.814190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.903027</td>\n",
       "      <td>0.501484</td>\n",
       "      <td>865478.000000</td>\n",
       "      <td>0.814466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>0.909437</td>\n",
       "      <td>0.493495</td>\n",
       "      <td>915336.000000</td>\n",
       "      <td>0.814154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.281300</td>\n",
       "      <td>0.905471</td>\n",
       "      <td>0.495709</td>\n",
       "      <td>967191.000000</td>\n",
       "      <td>0.814403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.365100</td>\n",
       "      <td>0.896485</td>\n",
       "      <td>0.506626</td>\n",
       "      <td>1018864.000000</td>\n",
       "      <td>0.814616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.174000</td>\n",
       "      <td>0.895502</td>\n",
       "      <td>0.504568</td>\n",
       "      <td>1070595.000000</td>\n",
       "      <td>0.814909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.898974</td>\n",
       "      <td>0.501691</td>\n",
       "      <td>1119856.000000</td>\n",
       "      <td>0.814799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.191400</td>\n",
       "      <td>0.907565</td>\n",
       "      <td>0.493221</td>\n",
       "      <td>1170324.000000</td>\n",
       "      <td>0.814532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.897258</td>\n",
       "      <td>0.500984</td>\n",
       "      <td>1222417.000000</td>\n",
       "      <td>0.814966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>0.908997</td>\n",
       "      <td>0.494809</td>\n",
       "      <td>1272854.000000</td>\n",
       "      <td>0.814599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>0.897266</td>\n",
       "      <td>0.498573</td>\n",
       "      <td>1322576.000000</td>\n",
       "      <td>0.815099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.907531</td>\n",
       "      <td>0.492579</td>\n",
       "      <td>1373796.000000</td>\n",
       "      <td>0.814621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.902894</td>\n",
       "      <td>0.495967</td>\n",
       "      <td>1423624.000000</td>\n",
       "      <td>0.814778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.898996</td>\n",
       "      <td>0.498740</td>\n",
       "      <td>1475125.000000</td>\n",
       "      <td>0.815149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.905457</td>\n",
       "      <td>0.496144</td>\n",
       "      <td>1528465.000000</td>\n",
       "      <td>0.814692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.900039</td>\n",
       "      <td>0.499542</td>\n",
       "      <td>1580384.000000</td>\n",
       "      <td>0.815121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.903567</td>\n",
       "      <td>0.497883</td>\n",
       "      <td>1630623.000000</td>\n",
       "      <td>0.814925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.902537</td>\n",
       "      <td>0.496883</td>\n",
       "      <td>1682295.000000</td>\n",
       "      <td>0.815060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.900173</td>\n",
       "      <td>0.498547</td>\n",
       "      <td>1734118.000000</td>\n",
       "      <td>0.814941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.901124</td>\n",
       "      <td>0.498440</td>\n",
       "      <td>1784567.000000</td>\n",
       "      <td>0.815128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.900659</td>\n",
       "      <td>0.498709</td>\n",
       "      <td>1837205.000000</td>\n",
       "      <td>0.815188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>0.901418</td>\n",
       "      <td>0.498083</td>\n",
       "      <td>1889288.000000</td>\n",
       "      <td>0.815226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig\n",
    "from trl.trainer import SFTTrainer, SFTConfig\n",
    "\n",
    "output_dir = 'outputs/Long-CoT-Math-Inference-Finetuning'\n",
    "run_name = 'Qwen2.5-0.5B-Long-CoT-gsm8k'\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    bias='none',\n",
    "    lora_dropout=0.01,\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=1e-4,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_length=512,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_checkpointing=True,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to='wandb',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1787974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /home/logan/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 18:29:32,389 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /home/logan/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 18:29:34,620 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "from numpy import dtype\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    './outputs/Long-CoT-Math-Inference-Finetuning/checkpoint-1869',\n",
    "    dtype=torch.bfloat16,\n",
    "    is_trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b0dafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. According to the question, please provide the user with detailed reasoning steps and answer in the following format:\n",
      "<reasoning>\n",
      "...\n",
      "</reasoning>\n",
      "<answer>\n",
      "...\n",
      "</answer>\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<reasoning>\n",
      "She has a total of 7 days because there are 365 / 52 = <<365/52=7>>7 days in a year.\n",
      "So, she has 7 * 16 = <<7*16=112>>112 eggs.\n",
      "She eats 3 + 4 = <<3+4=7>>7 eggs each day.\n",
      "So, she gives away 112 - 7 = <<112-7=95>>95 eggs.\n",
      "She makes 95 x 2 = $<<95*2=190>>190 from selling the eggs at the farmers' market.\n",
      "</reasoning>\n",
      "<answer>\n",
      "190\n",
      "</answer>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "sample = data['test'][0]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are Qwen, created by Alibaba Cloud. According to the question, please provide the user with detailed reasoning steps and answer in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n\"\"\"\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "        ]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors='pt').to(ft_model.device)\n",
    "\n",
    "generated_ids = ft_model.generate(**model_inputs, max_new_tokens=512)\n",
    "\n",
    "text = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
