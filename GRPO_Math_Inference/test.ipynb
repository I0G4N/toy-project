{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fa477b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:27:59,782 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:28:01,460 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    './outputs/Long-CoT-Math-Inference-Finetuning/checkpoint-1869',\n",
    "    dtype=torch.bfloat16,\n",
    "    is_trainable=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "rl_model = AutoModelForCausalLM.from_pretrained(\n",
    "    './outputs/Qwen2.5-0.5B-reasoning-GRPO/checkpoint-1868',\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b030a9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'message', 'prompt'],\n",
       "    num_rows: 1319\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are Qwen, created by Alibaba Cloud. According to the question, please provide the user with detailed reasoning steps and answer in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if '####' not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def create_prompt_formats(sample):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        sample['message'],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "    sample[\"prompt\"] = text\n",
    "\n",
    "    return sample\n",
    "\n",
    "def get_gsm8k_question(split='train'):\n",
    "    data = load_dataset(path='./../.cache/huggingface/datasets/gsm8k/main')[split]\n",
    "    data = data.map(lambda x: {\n",
    "        'message': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question'] + \"\\n\"}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # map the dataset to the new format\n",
    "    data = data.map(create_prompt_formats)\n",
    "    return data\n",
    "\n",
    "test_dataset = get_gsm8k_question('test')\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cbd37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strict_format_reward_func(prompts, completions, **kwargs):\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "\n",
    "    return [1.0 if match else 0.0 for match in matches]\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
    "    # strip for removing leading/trailing whitespace/newline\n",
    "    return answer\n",
    "\n",
    "def correctness_reward_func(prompts, completions, **kwargs):\n",
    "    # prompts [batch_size, conversation_turns, {role, content}]\n",
    "    # completions is a list of response dicts generated by the model [batch_size, [{role, content}]]\n",
    "    # answer [batch_size]\n",
    "    answer = kwargs.get('answer', [])\n",
    "    responses = [completion[0]['content'] for completion in completions] # get every response content from each completion\n",
    "    extracted_responses = [extract_xml_answer(response) for response in responses] # extract the answer from each response\n",
    "\n",
    "    # compare the extracted responses with the ground truth answers\n",
    "    return [1.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28948f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [51:07<00:00,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL Model - Strict Format Acc: 0.9697, Correctness Acc: 0.4428\n",
      "FT Model - Strict Format Acc: 0.0129, Correctness Acc: 0.2456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_batch_size = 4\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size)\n",
    "\n",
    "ft_model.eval()\n",
    "ft_format_reward = 0\n",
    "ft_correct_reward = 0\n",
    "\n",
    "rl_model.eval()\n",
    "rl_format_reward = 0\n",
    "rl_correct_reward = 0\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    prompts = batch['prompt']\n",
    "    answers = batch['answer']\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True, padding_side='left').to(ft_model.device)\n",
    "\n",
    "    ft_outputs = ft_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    completions = tokenizer.batch_decode(ft_outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    completions = [[{'role': 'assistant', 'content': c}] for c in completions]\n",
    "\n",
    "    ft_format_reward += sum(strict_format_reward_func(prompts, completions))\n",
    "    ft_correct_reward += sum(correctness_reward_func(prompts, completions, answer=answers))\n",
    "\n",
    "    rl_outputs = rl_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    rl_completions = tokenizer.batch_decode(rl_outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    rl_completions = [[{'role': 'assistant', 'content': c}] for c in rl_completions]\n",
    "\n",
    "    rl_format_reward += sum(strict_format_reward_func(prompts, rl_completions))\n",
    "    rl_correct_reward += sum(correctness_reward_func(prompts, rl_completions, answer=answers))\n",
    "\n",
    "print(f\"RL Model - Strict Format Acc: {rl_format_reward/len(test_dataset):.4f}, Correctness Acc: {rl_correct_reward/len(test_dataset):.4f}\")\n",
    "print(f\"FT Model - Strict Format Acc: {ft_format_reward/len(test_dataset):.4f}, Correctness Acc: {ft_correct_reward/len(test_dataset):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
